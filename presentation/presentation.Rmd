---
title: "Best subset selection"
author: "Rafał Muszyński, Ryszard Szymański"
date: "1/27/2020"
bibliography: bibliography.bib
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{dsfont}
   - \usepackage{algorithm2e}
   - \renewcommand{\tightlist}{\setlength{\itemsep}{5ex}\setlength{\parskip}{0pt}}
output: 
  beamer_presentation:
    theme: Warsaw
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Agenda

1. Problem introduction
2. Solution
3. Implementation
4. Benchmarks
5. Summary

## Introduction
Problem: feature selection in linear regression
\begin{equation}
\begin{split}
y = X \beta + \epsilon \\ 
\min_{\beta} ||y - X \beta||_2^2 
\end{split}
\end{equation}

Well known solution : Lasso: @tibshirani_regression_1996
\begin{equation}
\min_{\beta} \frac{1}{2}||y - X \beta||_2^2 + \lambda ||\beta||_1
\end{equation}

Testing findings of @bertsimas_best_2016 

## Motivation

- LASSO shortcomings e.g. larger coefficients are more penalized then smaller coefficients
- Advances in both hardware and optimization frameworks such as CPLEX and GUROBI
- Best subset selection is an NP-hard problem


## Solution
Restricted formulation of a MIQP problem.

\begin{equation}
    \begin{split}
        &\min \alpha^TQ\alpha + \alpha^Ta \\
        \text{s.t. } \quad &A\alpha \leq b \\
        &\alpha_i \in \{0,1\}, i \in \mathcal{I} \\
        &\alpha_i \in I\!R, i \notin \mathcal{I}\\
    \end{split}
\end{equation}


## Solution
Best subset selection problem formulation

\begin{equation}
\label{eq:bertsimas_problem}
\begin{split}
    &\min_{\beta} ||y - X \beta||_2^2 \\ 
    \text{s.t. } \quad &||\beta||_0  \leq k \\
    &||\beta||_0  = \sum_{i=1}^p 1(\beta_i \neq 0)\\
\end{split}
\end{equation}

## Solution
Final problem formulation
\begin{equation}
\begin{split}
&\min_{\beta, z} \frac{1}{2} \beta^{T}(X^{T} X) \beta-\langle X'y, \beta \rangle+\frac{1}{2}\|y\|_{2}^{2} \\
\text { s.t. } \quad &(\beta_{i}, 1 - z_{i}): \text{SOS-1}, \quad i=1, \ldots, p \\ 
&z_{i} \in\{0,1\}, \quad i=1, \ldots, p \\
&\sum_{i=1}^{p} z_{i} \leq k \\ 
&-\mathcal{M}_{U} \leq \beta_{i} \leq \mathcal{M}_{U}, \quad  i=1, \ldots, p  \\
\end{split}
\end{equation}

## Solution 
First order method
\SetKwRepeat{Do}{do}{while}
\begin{algorithm}[H]
\SetAlgoLined
\KwData{function: $g(\beta)$, parameter: L, convergence tolerance: $\epsilon$, parameter: k}
\KwResult{$\beta$ approximation}
 $\beta_1 \text{ random initialization, } \beta_1 \in I\!R, \|\beta\|_0 < k$\\
 \Do{$g(\beta_m) - g(\beta_{m+1}) \leq \epsilon$}{
    $\beta_{m+1} \in H_k(\beta_m - \frac{1}{L}\nabla g(\beta_m)) $
 }
 \caption{Discrete first-order method}
\end{algorithm}

## Implementation

- Three different starting methods:
  - Cold
  - Mild
  - Warm

- Two different solvers used:
  - CPLEX
  - GUROBI
  
- Parallelization for different $k$ values


## Benchmarks

- Both synthetic and real life datasets
- Syntethic datasets generated according to the procedure described in @bertsimas_best_2016
- Performed benchmarks analyzed
  - Predictive performance
  - Speed
  - Gap values for Warm/Mild/Cold start approaches
  
## Benchmarks - datasets

- Diabetes dataset
- Synthetic datasets
  - $x_i$ ~ $N(0, \Sigma)$, each standardized to have unit $l2$ norm
  - $y = X\beta^0 + \epsilon, \ \ \epsilon$ ~ $N(0, \sigma^2)$
  - The choice of $X, \beta^0, \epsilon$ determines the Signal-To-Noise Ratio: $SNR = \frac{var(X\beta^0)}{\sigma^2}$

## Benchmarks - predictive performance

\begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{../figures/predictive_performance_plot.pdf}
        \caption{Predictive performance of researched methods.}
        \label{fig:performance_plot}
\end{figure}


## Benchmarks - optimality gap for warm/cold starts

\begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{../figures/start-comparision.pdf}
        \caption{Optimality gap for warm, cold and mild start.}
        \label{fig:performance_plot}
\end{figure}


## Benchmarks - speed performance

\begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{../figures/speed_performance_plot.pdf}
        \caption{Speed of researched methods for datasets with a fixed number of variables (40) or observations (5000)}
        \label{fig:speed_plot}
\end{figure}


## Summary

- The MIO approach outperforms LARS in terms of predictive results
- The proposed mild approach allows for obtaining high quality results in times similar to LARS
- The best subset approach conversely to LASSO in a single run does not aim to maximally reduce the amoun of nonzero coefficients

## Bibliography


